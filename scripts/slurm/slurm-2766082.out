[nltk_data] Downloading package punkt to /gpfs/data/fs72332/fkovacev/d
[nltk_data]     ata/core/grapher/output/../lib/punkt...
[nltk_data] Downloading package punkt to /gpfs/data/fs72332/fkovacev/d
[nltk_data]     ata/core/grapher/output/../lib/punkt...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package punkt to /gpfs/data/fs72332/fkovacev/d
[nltk_data]     ata/core/grapher/output/../lib/punkt...
[nltk_data] Downloading package punkt to /gpfs/data/fs72332/fkovacev/d
[nltk_data]     ata/core/grapher/output/../lib/punkt...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data]   Package punkt is already up-to-date!
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-02-14 18:23:35 root INFO: Load training data into GraphDataset.
2024-02-14 18:23:35 root INFO: Load training data into GraphDataset.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-02-14 18:23:35 root INFO: Load training data into GraphDataset.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-02-14 18:23:35 root INFO: Load training data into GraphDataset.
2024-02-14 18:23:36 root INFO: Load validation data (dev) into GraphDataset.
2024-02-14 18:23:36 root INFO: Load validation data (dev) into GraphDataset.
2024-02-14 18:23:36 torch.distributed.nn.jit.instantiator INFO: Created a temporary directory at /tmp/tmpgnsdpcbs
2024-02-14 18:23:36 torch.distributed.nn.jit.instantiator INFO: Created a temporary directory at /tmp/tmpr_vpxbn7
2024-02-14 18:23:36 torch.distributed.nn.jit.instantiator INFO: Writing /tmp/tmpgnsdpcbs/_remote_module_non_scriptable.py
2024-02-14 18:23:36 torch.distributed.nn.jit.instantiator INFO: Writing /tmp/tmpr_vpxbn7/_remote_module_non_scriptable.py
2024-02-14 18:23:36 root INFO: Load validation data (dev) into GraphDataset.
2024-02-14 18:23:36 root INFO: Load validation data (dev) into GraphDataset.
2024-02-14 18:23:36 torch.distributed.nn.jit.instantiator INFO: Created a temporary directory at /tmp/tmp7mofec_u
2024-02-14 18:23:36 torch.distributed.nn.jit.instantiator INFO: Writing /tmp/tmp7mofec_u/_remote_module_non_scriptable.py
2024-02-14 18:23:36 torch.distributed.nn.jit.instantiator INFO: Created a temporary directory at /tmp/tmpysfsdybt
2024-02-14 18:23:36 torch.distributed.nn.jit.instantiator INFO: Writing /tmp/tmpysfsdybt/_remote_module_non_scriptable.py
2024-02-14 18:23:43 root INFO: NVIDIA A40
2024-02-14 18:23:43 root INFO: NVIDIA A40
2024-02-14 18:23:43 root INFO: NVIDIA A40
2024-02-14 18:23:43 root INFO: NVIDIA A40
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
`Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
2024-02-14 18:23:43 torch.distributed.distributed_c10d INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2024-02-14 18:23:43 torch.distributed.distributed_c10d INFO: Added key: store_based_barrier_key:1 to store for rank: 2
2024-02-14 18:23:43 torch.distributed.distributed_c10d INFO: Added key: store_based_barrier_key:1 to store for rank: 3
2024-02-14 18:23:43 torch.distributed.distributed_c10d INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2024-02-14 18:23:43 torch.distributed.distributed_c10d INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2024-02-14 18:23:43 torch.distributed.distributed_c10d INFO: Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

2024-02-14 18:23:43 torch.distributed.distributed_c10d INFO: Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2024-02-14 18:23:43 torch.distributed.distributed_c10d INFO: Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2024-02-14 18:23:46 root INFO: Load training data into GraphDataset.
2024-02-14 18:23:46 root INFO: Load training data into GraphDataset.
2024-02-14 18:23:46 root INFO: Load training data into GraphDataset.
2024-02-14 18:23:46 root INFO: Load training data into GraphDataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name  ┃ Type    ┃ Params ┃
┡━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━━┩
│ 0 │ model │ Grapher │  809 M │
└───┴───────┴─────────┴────────┘
Trainable params: 809 M                                                         
Non-trainable params: 0                                                         
Total params: 809 M                                                             
Total estimated model params size (MB): 3.2 K                                   
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
/gpfs/data/fs72332/fkovacev/miniconda3/envs/data_science/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/gpfs/data/fs72332/fkovacev/miniconda3/envs/data_science/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/gpfs/data/fs72332/fkovacev/miniconda3/envs/data_science/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/gpfs/data/fs72332/fkovacev/miniconda3/envs/data_science/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2024-02-14 18:24:04 torch.nn.parallel.distributed INFO: Reducer buckets have been rebuilt in this iteration.
2024-02-14 18:24:04 torch.nn.parallel.distributed INFO: Reducer buckets have been rebuilt in this iteration.
2024-02-14 18:24:04 torch.nn.parallel.distributed INFO: Reducer buckets have been rebuilt in this iteration.
2024-02-14 18:24:04 torch.nn.parallel.distributed INFO: Reducer buckets have been rebuilt in this iteration.
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 2766082.0 ON n3066-015 CANCELLED AT 2024-02-14T18:24:53 ***
slurmstepd: error: *** JOB 2766082 ON n3066-015 CANCELLED AT 2024-02-14T18:24:53 ***
